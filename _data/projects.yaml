# Projects
- project: Managed Data Pipelines
  role: Developer
  duration: 2022 &mdash; Present
  #description: 코인원의 데이터 웨어하우스로 사용중인 BigQuery 로 데이터를 적재하는 파이프라인을 개발하고 운영했습니다.
                #Airflow 의 Custom Operator 를 활용해 EL 과정을 개발하였고, 다양한 소스(CDC, ElasticSearch)를 BigQuery 로 적재하여 사내 데이터 환경을 개선했습니다.
                #Airflow 는 AWS EKS 위 Helm & ArgoCD 를 이용해 배포하였고, kubernetes native 한 workflow 를 도입하고 있습니다.
  description: Developed and operated a pipeline to load data into <a href="https://cloud.google.com/bigquery" target="_blank" style="text-decoration:none; border-bottom:none;">BigQuery</a>.
               Using Airflow Custom Operator, developed EL steps and improved our data environment by various sources (CDC, ElasticSearch) into BigQuery.
               Airflow is deployed on AWS EKS using Helm and ArgoCD, and introduces kubernetes native workflows.
#  troubleshoots:
#    - troubleshoot1
#    - troubleshoot2

- project: Optimizing dbt pipeline with Slim CI
  role: Developer
  duration: 2022 Sept &mdash; 2022 Dec
  #description: 전사 지표를 제공하기 위해 dbt 를 이용하고 있다. dbt 사용자들에게 sqllint, incident to jira 등 환경을 개선하고 있다.
                #궁극적인 목표는 사용자가 환경에 구애받지 않고 자유롭게 지표를 개발할 수 있도록 하는 것이다.
  #description: 전사 지표를 제공하기 위해 dbt 를 이용하고 있다. 변경된 모델만 빌드할 수 있도록 Slim CI 를 적용했다.
                #manifest 파일은 AWS S3 에 업로드하여 관리했고, 브랜치 푸시 이벤트가 발생하면 기존 manifest 와 비교하며 변경모델만 빌드한다.
                #기존 모든 모델을 빌드하던 시간을 3배 이상 개선하였다. (203s -> 77s)
  description: I used dbt to provide Metric Store, and applied Slim CI.
               Manifest file is managed by AWS S3, and when a branch push event occurs, it's compared to the existed manifest and only the change model is built.
               Improved model build time by more than 3x (203s -> 77s). <a href="https://jx2lee.netlify.app/data/dbt/__/dbt-cicd-overview" target="_blank">See more</a>.
#  troubleshoots:
#    - troubleshoot1
#    - troubleshoot2

- project: Self-Serve-Data
  role: Developer
  duration: 2022 Sep &mdash; 2023 Jun
  #description: 데이터에 의한 의사결정을 위해, 손쉽게 데이터를 확인할 수 있는 도구들을 배포하고 운영했다.
                #데이터 카탈로그 서비스인 Datahub 와, API 데이터 수집용으로 쉽게 connector 를 개발할 수 있는 Airbyte 를 배포했다.
                #Airbyte 의 경우 oss 버전에서는 oauth 기능을 제공하지 않아 oauth2-proxy 를 이용해 권한 관리를 가능하게 했다.
                #누구나 쉽게 데이터를 확인할 수 있도록 다양한 도구들을 제공할 예정이다.
  description: In order to make data-driven decisions, I deployed and operated tools that made it easy to see the data (self-serve).
               I deployed Datahub, open source data catalog, and Airbyte which allows you to easily develop connectors for API data.
               In the case of Airbyte, the oss version does not provided OAuth, so I used oauth2-proxy to enable authorization
               (<a href="https://jx2lee.netlify.app/data/airbyte/__/airbyte-with-oauth2-proxy">see more</a>).
               I'll plan to provide various tools so that anyone can easily check data.
#  troubleshoots:
#    - troubleshoot1
#    - troubleshoot2

- project: Built the Elasticsearch to BigQuery Pipeline
  role: Developer
  duration: 2023 July &mdash; 2023 Sep
  #description: Airbyte 에서 제공하는 HTTP 커넥터 빌더를 이용해 Elasticsearch to BigQuery 파이프라인 구축했다.
                #로그인 프로세스 개선을 위해 elasticsearch 로그를 Airbyte CDK(Connector Development Kit) 를 활용해 데이터웨어하우스로 적재했다.
                #Airbyte 에서 제공하는 커넥터는 Full Table Replication 방식이지만, _id & timestamp 기반 cursor_field 로 incremental Append 모드를 제공했다.
                #쿼리 기반으로 데이터를 수집하기 때문에 사용자가 원하는 쿼리로 수집할 수 있도록 개선이 필요하다.
                #<a href="https://jx2lee.netlify.app/data/airbyte/__/airbyte-cdk-elasticsearch-rest/">See more.</a>
  description: I used the HTTP connector builder provided by Airbyte to build an Elasticsearch to BigQuery pipeline.
               To improve the login process, extracted and loaded elasticsearch logs into BigQuery using Airbyte CDK, Connector Development Kit.
               The existed connector provided full table replication, but I added incremental append mode with cursor_field based on "_id & timestamp".
               <a href="https://jx2lee.netlify.app/data/airbyte/__/airbyte-cdk-elasticsearch-rest">See more.</a>
#  troubleshoots:
#    - troubleshoot1
#    - troubleshoot2

- project: Migrated from BitBucket dbt CI/CD to GitHub Actions
  role: Developer
  duration: 2023 Jul &mdash; 2023 Sep
  #description: Atlassian bamboo 로 운영중인 dbt CI/CD 파이프라인을 Github Actions 로 이관했다.
                #기존 파이프라인 기능 유지 및 Github Actions 를 활용한 워크플로우를 추가했다. (Pull Reqeust checker/labeler, sqlfluff(linter), manual dbt build)
                #로컬에서 특정모델을 빌드하지 않고 workflow_dispatch 를 활용해 작업자 간 크로스체크 가능한 환경 등을 제공하며 dbt 사용자 경험을 향상시켰다.
                #<a href="https://jx2lee.netlify.app/data/dbt/__/dbt-cicd-pipeline">See more.</a>
  description: Migrated dbt CI/CD pipeline running on Atlassian bamboo to Github Actions.
               Maintained the existing pipeline functionality and added workflows with Github Actions Workflow.
               (Pull Request checker/labeler, sqlfluff(linter), manual dbt build)
               Improved DBT user experience by not building specific models locally and utilizing workflow_dispatch to provide a cross-checkable environment between crews.
               <a href="https://jx2lee.netlify.app/data/dbt/__/dbt-cicd-pipeline">See more.</a>
#  troubleshoots:
#    - troubleshoot1
#    - troubleshoot2

- project: Built streaming pipeline using Kafka Connect
  role: Developer
  duration: 2023 Sep &mdash; Dec
  #description: Kafka Connect 를 이용해 서버로그 파이프라인을 구축했다.
                #기존 Kinesis & lambda 실시간 파이프라인을 BigQuery Sink Connector 를 이용해 BigQuery 로 적재하도록 이관했다.
                #매니지드 서비스 미사용(kinesis, lambda)으로 비용을 절감했다.
                #현재 메세지 수집 실패에 대한 재처리 로직을 설계중이며 grafana 로 커넥터 클러스터 모니터링을 구성하고 있다.
  description: Built a server log pipeline using Kafka Connect.
               Migrated existing Kinesis & lambda streaming pipeline to Kafka Connect using BigQuery Sink Connector.
               Reduced costs by removing managed-services (kinesis, lambda),
               Designed reprocessing logic for message collection failures and configuring connector cluster monitoring with Kibana.
#  troubleshoots:
#    - troubleshoot1
#    - troubleshoot2
